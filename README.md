## ***ğŸ›’ Demand Forecasting using AWS SageMaker & XGBoost***
ğŸ”— GitHub Repository: demand-forecasting ( [Demand Forecasting Project](https://github.com/himanshu-dandle/demand-forecasting)  )

## ğŸ“ˆ Project Overview:
This project builds a demand forecasting model using XGBoost, deployed on AWS SageMaker with a CI/CD pipeline using GitHub Actions.

âœ… Key Features:
**Data Preprocessing & Feature Engineering 
âœ” **Model Training with XGBoost ğŸ“Š
âœ” **Automated Deployment to AWS SageMaker ğŸš€
âœ” **Real-time Inference via API ğŸ”®
âœ” **CI/CD Pipeline for Model Deployment ğŸ”„


### Why Demand Forecasting?  
 **Reduces inventory costs** by minimizing overstock and shortages  
 **Improves decision-making** for procurement and logistics  
 **Enhances customer satisfaction** with better product availability
 

## Dataset Details
***The dataset used in this project comes from **Kaggle** and contains historical sales data for demand forecasting.

***ğŸ“Œ Source: Kaggle - Demand Forecasting Dataset

The dataset includes:

***record_ID â€“ Unique record identifier
***week â€“ Weekly sales data
***store_id â€“ Store identifier
***sku_id â€“ Stock Keeping Unit (SKU) identifier
***total_price â€“ Total revenue generated
***units_sold â€“ Target variable (demand forecast)
âœ… Data Preprocessing Includes:
âœ” ***Handling Missing Values & Outliers
âœ” ***Feature Engineering (Year, Month, Week Number, etc.)
âœ” ***Scaling & Encoding for Model Training



### Key Features  
 **Data Cleaning & Feature Engineering** (date-based & categorical features)  
 **ML Model Training & Evaluation** (XGBoost, Random Forest, Gradient Boosting)  
 **AWS SageMaker Deployment** (for scalable predictions)  
 **Automated CI/CD Pipeline** (GitHub Actions for continuous deployment)  

---

## ğŸ“‚ Project Structure  
*** demand-forecasting/
***â”‚â”€â”€ data/                   # Raw & processed datasets  
***â”‚â”€â”€ models/                 # Trained models & artifacts  
***â”‚â”€â”€ notebooks/              # Jupyter Notebooks for EDA & training  
***â”‚â”€â”€ plots/                  # Visualizations  
***â”‚â”€â”€ src/                    # Python scripts for processing & training  
***â”‚   â”œâ”€â”€ 04_model_training.py       # Train & save the XGBoost model  
***â”‚   â”œâ”€â”€ 05_deployment_testing.py   # Deploy & test inference on SageMaker  
***â”‚   â”œâ”€â”€ test_inference.py          # Run batch inference  
***â”‚â”€â”€ deployment/             # Deployment configurations  
***â”‚â”€â”€ .github/workflows/      # GitHub Actions CI/CD  
***â”‚â”€â”€ .env                    # AWS credentials (ignored in GitHub)  
â”‚***â”€â”€ README.md               # Project documentation  


---

##  Model Performance Evaluation  

| Model               	 	| MAE       	| MSE        | RÂ²        |
|----------------------|--------------------|------------|-----------|
| **XGBoost**         		| 0.3738 	 	| 0.2518     | **0.7462**|
| **Random Forest**   		| 0.3755        | 0.2721     | 0.7258    |
| **Gradient Boosting** 	| 0.5735        | 0.5554     | 0.4403    |

 **XGBoost performed the best** and was selected for deployment.  

---

##  Technologies Used  
- **Programming:** Python (pandas, numpy, scikit-learn, XGBoost)  
- **Notebook Environment:** Jupyter Notebook  
- **Cloud Platform:** AWS SageMaker (for deployment)  
- **Storage:** Amazon S3 (for model storage)  
- **CI/CD Pipeline:** GitHub Actions (for automatic deployment)  

---

##  ğŸ¯ How to Run This Project  

### 1ï¸ Clone the Repository  
```
git clone https://github.com/himanshu-dandle/demand-forecasting.git
cd Demand_Forecasting_Project
```

### 2ï¸ Set Up Virtual Environment & Install Dependencies

python -m venv forecasting_env
source forecasting_env/bin/activate  # Mac/Linux
forecasting_env\Scripts\activate     # Windows
pip install -r requirements.txt

### 3ï¸ configure AWS Credentials
Create a .env file in the project root (DO NOT COMMIT THIS FILE).
	SAGEMAKER_BUCKET=demand-forecasting-bucket-us-east-1
	SAGEMAKER_ROLE=arn:aws:iam::060795905003:role/service-role/AmazonSageMaker-ExecutionRole-XXXXX

###4ï¸ Train the Model
	
	python src/04_model_training.py
	
###5ï¸ Deploy the Model to AWS SageMaker
	python src/05_deployment_testing.py


###6. Making Predictions with the Deployed Model
   python src/test_inference.py

##ğŸš€ Deployment Pipeline (AWS SageMaker & CI/CD)

**This project includes automated model deployment via GitHub Actions.

 Workflow File: .github/workflows/deploy.yml
 Triggers: Runs on every push to main branch

**ğŸ”„ What Happens in CI/CD?
***Checks out the latest code from GitHub
***Installs dependencies inside a virtual environment
***Deploys the trained model to AWS SageMaker
***Runs inference tests to verify deployment


**Environment Variables & Secrets
This project uses GitHub Secrets for secure access to AWS credentials:

**Secret Name	Description
**AWS_ACCESS_KEY_ID		AWS Access Key for authentication
**AWS_SECRET_ACCESS_KEY	AWS Secret Key for authentication
**SAGEMAKER_BUCKET		S3 bucket name for storing model files
**SAGEMAKER_ROLE			AWS IAM Role for SageMaker execution

**ğŸ“Œ To set up GitHub Secrets:

***Go to your GitHub repo â†’ "Settings" â†’ "Secrets and variables" â†’ "Actions"
***Add the required secrets (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, etc.)


***ğŸ”§ Deployment Steps
**Model Training (04_model_training.py)
		Trains XGBoost model & saves it as xgboost_model.pkl
***Model Packaging (05_deployment_testing.py)
	**Converts .pkl to SageMaker-compatible .model
	**Packages the model into xgboost_model.tar.gz
	**Uploads to AWS S3
	**AWS SageMaker Deployment
	**Creates a SageMaker Model
	**Deploys an Inference Endpoint
	**Waits for deployment to be InService
	**GitHub Actions CI/CD Pipeline
	**Triggers deployment on every push to main branch
	**Deletes old models, endpoints & redeploys the latest version

***GitHub Actions Workflow:
```
name: SageMaker Deployment Pipeline

on:
  push:
    branches:
      - main

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: ğŸš€ Checkout Repository
        uses: actions/checkout@v3

      - name: ğŸ Set Up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'

      - name: ğŸ“¦ Install Dependencies
        run: |
          python -m venv forecasting_env
          source forecasting_env/bin/activate
          pip install --no-cache-dir -r requirements.txt
          pip install --upgrade boto3 sagemaker

      - name: ğŸ”‘ Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: ğŸ—ï¸ Deploy Model to SageMaker
        run: |
          source forecasting_env/bin/activate
          python src/05_deployment_testing.py

      - name: âœ… Deployment Completed
        run: echo "Deployment to AWS SageMaker is successful!"
```

##ğŸ”¬ Model Inference (Real-Time Predictions)
Once deployed, predictions can be made using the SageMaker Inference API

***1ï¸. Invoke Endpoint Using AWS CLI
```
	aws sagemaker-runtime invoke-endpoint \
    --endpoint-name xgboost-demand-forecasting-endpoint \
    --content-type "text/csv" \
    --body "5.2,3.1,1.4,0.2" \
    output.json --region us-east-1
```
***2.Invoke Endpoint Using Python
```
import boto3

ENDPOINT_NAME = "xgboost-demand-forecasting-endpoint"
runtime = boto3.client("sagemaker-runtime")

test_data = "5.2,3.1,1.4,0.2\n"  # CSV format

response = runtime.invoke_endpoint(
    EndpointName=ENDPOINT_NAME,
    ContentType="text/csv",
    Body=test_data
)

result = response["Body"].read().decode("utf-8")
print("âœ… Model Inference Output:", result)
```


##Future Enhancements
***Hyperparameter Tuning: Optimize model parameters for better 
***Feature Engineering Improvements (Lags, Moving Averages)
***Deep Learning Models: Implement LSTMs or Transformers for time series forecasting
***External Data Sources: Include macroeconomic indicators for better predictions
***Deploy as API: Use FastAPI or Flask for real-time predictions
***Scale Deployment with AWS Lambda

##Troubleshooting Guide
***Issue: Deployment fails with ModelError (415) - application/json is not an accepted ContentType
***Fix: Update test_inference.py to use "text/csv" instead of "application/json".

***Issue: FileNotFoundError: models/xgboost_model.pkl not found in CI/CD
***Fix: Either commit the .pkl file or upload it to S3 before deployment.

***Issue: Model is InService, but inference fails
***Fix: Check AWS CloudWatch Logs for error details.



***Contact & Contributions
***Author: Himanshu Dandle
***Email: HIMANSHU.DANDLE#GMAIL.COM
***GitHub: himanshu-dandle